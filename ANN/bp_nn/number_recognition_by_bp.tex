\documentclass{article}
\usepackage{xeCJK}
%\setCJKmainfont{KaiTi}
\usepackage{amsmath, bm}
\usepackage{listings}
\include{lstlang_racket.sty}
\usepackage{color}
\usepackage{textcomp}
\usepackage{geometry}
\usepackage{graphicx}

\begin{document}

\title{基于BP神经网络的手写数字识别}
\author{张思言}
\date{2018/12/01}
\maketitle

  \begin{center}
    \section*{摘要}
  \end{center}
    为了深入了解BP神经网络，了解BP神经网络在图像识别方面的应用，我们编写了BP神经网络进行了测试，采用mnist手写数字数据集作为训练集以及测试集．我们分别研究了步长，批量大小（batch size），网络结构对神经网络效果的影响．

  \newpage

  \section{正文}

  \subsection{问题建模}
    我们对手写数字识别这个任务进行了数学建模：
  \begin{enumerate}

    \item 选择激活函数

      我们选择了sigmoid函数：
      \begin{equation}
        S(x) = \frac{1}{1+e^{-x}}
      \end{equation}

    \item 输入层的建模

      mnist数据集的图片大小为$28\cdot 28$，我们将图片数字化，按从上到下，从左到右的顺序放入一个向量$\bm{x}$中，这个向量有$28\cdot28$（784）维，对应着输入层的784个神经元．并且由于图片的每个像素为灰度值（$0\sim255$），为了方便神经网络的计算和收敛，我们通过除以255来进行简单的归一化处理．

    \item 输出层的建模

      输出层有十个神经元，每个神经元代表一个数字，如果某个神经元为1，其余神经元为0，则代表这个神经元所对应的数字．在使用网络测试时，由于输出的值在$0\sim1$的范围内，我们选择值最大的那个神经元对应的数字作为结果来判断此次预测是否正确．

    \item 误差函数的选择

      我们选择了$\frac{1}{10}\sum_{i=1}^{10}{\frac{1}{2} (y_i - t_i) ^2}$作为误差函数进行评判．
  \end{enumerate}

  \subsection{网络结构的影响}
  据研究，增加网络层数的方法在这次实验中的效果不如增加中间层神经元的数量，所以我们选用三层网络结构，对中间层的神经元的数量进行修改，并查看其影响．下面是在同样的批量，学习率下，隐含层神经元数量分别为10，100，200时的效果，横坐标为用于训练的样本的批量．

  \newgeometry{left=1cm, right=1cm, top=1cm, bottom=1.5cm}
  \begin{figure}
    \centering
    \includegraphics[scale=0.6]{result/loss_struct_mid10.png}
    \includegraphics[scale=0.6]{result/suc_struct_mid10.png}
    \caption{隐含层神经元数量为10时的误差值(左)及测试成功率(右)}
  \end{figure}
  \begin{figure}
    \centering
    \includegraphics[scale=0.6]{result/loss_struct_mid100.png}
    \includegraphics[scale=0.6]{result/suc_struct_mid100.png}
    \caption{隐含层神经元数量为100时的误差值(左)及测试成功率(右)}
  \end{figure}
  \begin{figure}
    \centering
    \includegraphics[scale=0.6]{result/loss_struct_mid200.png}
    \includegraphics[scale=0.6]{result/suc_struct_mid200.png}
    \caption{隐含层神经元数量为200时的误差值(左)及测试成功率(右)}
  \end{figure}
  \restoregeometry

  我们发现，隐藏层神经元的个数越多收敛的速度就越快，但是这个速度是用批量的数量计算的（即每放入一个批量误差下降的量），实际上神经元个数越多计算复杂度越大，实际计算所需要的时间可能更多．所以隐藏层神经元的数量应该适量．

  所以，根据经验公式$n_h = \sqrt{n_i + n_o} + 5\sim10$计算处中间层神经元的数量大概在70左右．下面的实验都是基于这个网络结构的．


  \subsection{批量大小以及步长综合的影响}

  \subsubsection{SGD, BGD的理论分析}

  \begin{itemize}
    \item SGD(Stochastic Gradient Decrease)指的是随机梯度下降法．随机梯度下降法每训练一个样本就用来更新一次网络，具有收敛快的优点，但是由于单个样本的噪声比较大，结果具有随机性，不一定会趋向全局最优解，而且不容易实现并行运算．

    \item BGD(Batch Gradient Decrease)指的是批量梯度下降法，指的是每次更新网络都用n个样本，其中n指的是批量大小．这种方法改进了随机梯度下降法的缺点，能够较好地趋向全局最优解，而且便于并行运算，但是收敛速度慢．
  \end{itemize}

  \subsubsection{SGD与BGD对照实验}
  下面分别是用随机梯度下降法和批量梯度法的结果（误差值的横坐标为放入网络训练的批量，而成功率的横坐标为放入网络训练的样本数量）：
  \newgeometry{left=1cm, right=1cm, top=1cm, bottom=1.5cm}
  \begin{figure}
    \centering
    \includegraphics[scale=0.6]{result/mean_error.png}
    \includegraphics[scale=0.6]{result/success_rate.png}
    \caption{随机梯度法：误差值(左)及测试成功率(右)}
  \end{figure}
  \begin{figure}
    \centering
    \includegraphics[scale=0.6]{result/loss_bgd.png}
    \includegraphics[scale=0.6]{result/suc_bgd.png}
    \caption{批量梯度法(批量为50)：误差值(左)及测试成功率(右)}
  \end{figure}
  \restoregeometry

  在实验过程中，我们发现超过了50的批量，它们的测试成功率稳定在0.1135，误差稳定在0.045左右，然后就难以下降了（在我们实验的笔记本上几个小时都几乎不变）．由于神经网络训练耗时太长，我们选择了尽量大的步长，使之能够尽快地收敛．

  可以看出使用随机梯度下降法具有较大的随机性，网络收敛以后，随机梯度下降法测试的正确率任然在不停地波动．而使用批量梯度法的网络一旦收敛于某一处，继续训练的结果长时间（几个小时）也不会发生太大变化．可以看出批量梯度法的测试成功率非常的低，反而不如随机梯度法，猜测可能是因为陷入马鞍面而导致梯度的消失，或者是陷入一块梯度很小的"平地"．

  实践表明，随机梯度虽然可能会导致局部最优，但是机器学习中局部最优也有可能效果不错，而且其泛化能力强，不容易过拟合．而且具有收敛速度快，能够逃离马鞍面等种种优点．所以使用随机梯度法是进行机器学习中一种常用的方法．
  \subsubsection{MBGD}
    为了结合随机梯度法下降速度快，批量梯度法能稳定地趋向全局最优的优点，我们使用了小批量梯度下降法(MBGD: Mini-batch Gradient Decrease)进行尝试．
    小批量梯度下降法指的是将批量梯度下降法中的批量大小限制到比较小（一般在10以下）的方法．批量大小的确定没有定论，需要根据实际一一尝试，所以我们进行了实验．下面是批量分别为2, 4, 8时的测试成果率(步长为0.3)：
  \newgeometry{left=1cm, right=1cm, top=1cm, bottom=1.5cm}
  \begin{figure}
    \centering
    \includegraphics[scale=0.5]{result/loss_mbgd2.png}
    \includegraphics[scale=0.6]{result/suc_mbgd2.png}
    \caption{批量为2：误差值(上)及测试成功率(下)}
  \end{figure}
  \begin{figure}
    \centering
    \includegraphics[scale=0.6]{result/loss_mbgd4.png}
    \includegraphics[scale=0.6]{result/suc_mbgd4.png}
    \caption{批量为4：误差值(左)及测试成功率(右)}
  \end{figure}
  \begin{figure}
    \centering
    \includegraphics[scale=0.6]{result/loss_mbgd8.png}
    \includegraphics[scale=0.6]{result/suc_mbgd8.png}
    \caption{批量为8：误差值(左)及测试成功率(右)}
  \end{figure}
  \restoregeometry
    虽然不太明显，但是可以看出批量越小，测试成功率上升地越快，而波动却越大，批量越大，成功率上升地越慢，但是越稳定．可见，训练速度与稳定性是一对矛盾体，这与控制论中系统的稳定性和收敛速度是矛盾体这一点不谋而合．

    前文所述每个神经网络训练的时间都基本差不多，所以使用小批量梯度法的效果比起前面的各种尝试可以说有了非常大的进步，效果有了明显的提升，最终成功率已经稳定在0.92左右．

  \section{小结}
  单纯用BP神经网络进行图像的识别的效果不太好，而且BP神经网络步长的调整也不太方便，应该结合卷积神经网络进行相关研究．以及步长的手动调整也非常不方便，可以研究自动调整步长的算法．另外批量梯度法也可以和随机梯度法结合起来，在批量梯度法中穿插着随机梯度法，这样就可以达到既能逃离马鞍面，较快地收敛，又能保持稳定的效果，而且也能使并行计算成为可能，加快训练速度．

  \section{代码实现}

  \newgeometry{left=1cm, right=1cm, top=1cm, bottom=1.5cm}
  \lstset{language=Racket}
  \begin{lstlisting}
#! /usr/bin/env racket
#lang racket
(require "../Lisp/libcommon.rkt")
(require csv-reading)
(require racket/fasl)

(provide make-network apply-network final-output
         (contract-out [train (->* (string? string? list?) (#:learning-rate number?
                                                        #:act-f procedure?
                                                        #:precision number?) list?)]))
(provide set-train-times)
(provide set-α set-batch-size)
(provide ntw-layer ntw-node ntw-w add-network)
(provide transpose sigmoid dot-prod scale remove-head remove-tail)

(def make-data-reader
     (make-csv-reader-maker
       '((separator-chars            #\,)
         (strip-leading-whitespace?  . #t)
         (strip-trailing-whitespace? . #t))))

(def (row->label row)
  (def (iter num i res)
    (if (= i 0)
      (reverse res)
      (if (= (- 10 num) i)
        (iter num (- i 1) (cons 1 res))
        (iter num (- i 1) (cons 0 res)))))
  (iter (string->number (first row)) 10 null))

; 根据wikipedia的公式而写
; 输出层应该只有一个neuron

(def (train-bp1 input t network
                #:learning-rate η
                #:act-f af
                #:precision p)

  ; get-δ {{{
  (def (get-δ o-nh t_ ntw-nh) ; -nh represents no head
    (let ([layer-o (car o-nh)])
      (if (null? ntw-nh)                    ; output layer
        (cons (map (λ (elem-o elem-t)
                     (* (- elem-o elem-t)  ; use sigmoid function
                        elem-o
                        (- 1 elem-o)))
                   layer-o
                   t_)
              null)
        (let ([layer-ntw (car ntw-nh)]
              [next-δ (get-δ (cdr o-nh) t_ (cdr ntw-nh))])     ; inner layer
          (cons (map (λ (elem-o elem-ntw)
                       (* elem-o
                          (- 1 elem-o)
                          (dot-prod
                            elem-ntw
                            (first next-δ))))
                     layer-o
                     (transpose layer-ntw))
                next-δ)))))
  ; }}}

  ; get-Δw {{{
  (def (get-Δw o t_ ntw)
    ;(displayln (get-Δw (remove-head o) t_ (remove-head ntw)))
    (map (λ (layer-δ lst-o)
           (map (λ (elem-δ)
                  (scale (* -1 η elem-δ) lst-o))
                layer-δ))
         (get-δ (remove-head o) t_ (remove-head ntw))
         (remove-tail o)))
  ; }}}

  ; t_ should be numbers; o should be result of apply-network
  ; ntw is a mpair, [a,b) is the range of i_ and t_ that will be used.]
  (def (new-network i_ t_ ind a b)
    ;(display a) (display ", ") (displayln b)
    (let* ([_i (take (list-tail i_ a) (- b a))]
           [_t (take (list-tail t_ a) (- b a))]
           [sum-Δw (foldl add-network #f
                          (map (λ (i__ t__)
                                 (get-Δw (apply-network i__ network #:act-f af)
                                         t__ network))
                               _i
                               _t))])
      ;(display "_t equal t_? ") (displayln (equal? _t t_))
      ;(display sum-Δw)
      (vector-set! batch-delat-w ind sum-Δw)))

  ; analyze-error {{{
  (def (analyze-error i_ t_)
    (let
      ([mean-error  ; err = average(1/2 * (yi - ti)^2), i = 0,1,...,9
         (average
           (map (λ (input true-value)
                  (let ([output (final-output (apply-network input network))])
                    ;(map (λ (o__ t__) (when (= t__ 1) (displayln (apply max output))(display o__) (display "\t") (displayln t__))) output true-value)
                    ;(displayln "*************************")
                    (average (map loss-func output true-value))))
                i_
                t_))]
       [abs-error (average
           (map (λ (input true-value)
                  (let ([output (final-output (apply-network input network))])
                    (average (map (λ (y t) (abs (- y t))) output true-value))))
                i_
                t_))])
      (display "Train No. ") (display (+ 1 (- old-max-train-times max-train-times)))
      (display "\tmean error: ") (display mean-error)
      (display "\t\tabs error: ") (displayln abs-error)))
  ; }}}

  (def core-num 1)
  (display "using ") (display core-num) (displayln " cores of cpu...")
  (def batch-delat-w (make-vector core-num))
  (def (train-batch i_ t_)
    (for ([ind (build-list core-num values)])
      ;(display "FOR: ")
      ;(displayln ind)
      (new-network i_ t_ ind
                   (round (* (/ ind core-num)
                             batch-size))
                   (round (* (/ (+ ind 1) core-num)
                             batch-size))))
    (set! network (foldl add-network network (vector->list batch-delat-w)))
    (analyze-error i_ t_))

  (def old-max-train-times max-train-times)
  ;(when (not (file-exists? "ntws"))
  ;(with-output-to-file "ntws" (λ () (s-exp->fasl null (current-output-port)))))
  (def (loop)
    (if (= max-train-times 0)
      null
      (let* ([reader-t (make-data-reader (open-input-file t))]
             [reader-i (make-data-reader (open-input-file input))]
             ;[ntws (with-input-from-file "ntws" (λ () (fasl->s-exp (current-input-port))))]
             )
        ;(when (not (null? ntws)) (set! network (car ntws)))

        (def (iter i)
          (when (= 0 (remainder (* i batch-size) 2000))
            (analyze-result (csv->data  "./dataset/test-images.csv")
                            (csv->label "./dataset/test-labels.csv") network))
          (let ([i_raw (csv-take reader-i batch-size)]
                [t_raw (csv-take reader-t batch-size)])
            (if (< (len i_raw) batch-size)
              (begin
                (set! max-train-times (- max-train-times 1))
                )
              (let ([i_ (mapmap (λ (s) (- (string->number s) 0.5)) i_raw)]
                    [t_ (map row->label t_raw)])
                (train-batch i_ t_)
                ;(set! max-train-times (- max-train-times 1))
                (iter (+ i 1))
                ))))
        (iter 0)
        ;(with-output-to-file "ntws" #:exists 'replace
                             ;(λ () (s-exp->fasl (cons network ntws)
                                                ;(current-output-port))))
        (loop))))
  (loop)
  network)

; (csv->data str) {{{
(def (csv->data str)
  (csv-map (lambda (row)
             (map (λ (s) (- (string->number s) 0.5)) row))
           (make-data-reader (open-input-file str))))
; }}}
; (csv->label str) {{{
(def (csv->label str)
  (def (iter one i res)
    (if (= i 0)
      (reverse res)
      (if (= (- 10 one) i)
        (iter one (- i 1) (cons 1 res))
        (iter one (- i 1) (cons 0 res)))))
  (csv-map (lambda (row)
             (iter (string->number (first row)) 10 null))
           (open-input-file str)))
; }}}
(def (analyze-result test-samples test-labels ntw)
  (newline) (displayln "test starting...") (newline)
  (let* ([outputs (map (λ (input) (final-output (apply-network input ntw)))
                       test-samples)]
         ;[n (displayln (car outputs))]
         [result (map (λ (output t)
                        (let* ([max-output (apply max output)]
                               [norm-output (map (λ (x) (if (= x max-output) 1 0))
                                                 output)])
                          (if (equal? norm-output t)
                            1
                            0)))
                      outputs
                      test-labels)]
         [success-rate (average result)])
    (displayln (take result 10))
    (display "success rate: ") (display success-rate) (display " (")
    (display (exact->inexact success-rate)) (displayln ")"))

  (displayln "test finished"))
; network API {{{

(def max-train-times 1000000000)
; decide how many mean error during the training will be show
(def show-err 100) 
(def show-part-num 100)
(def α 0.5)
(def batch-size 50)
;; 每个神经元都假定与前一层的全部神经元相连
;; 构造出来的网络实际上是权值(w)
(def (set-train-times n) (set! max-train-times n))
;(def (set-show-err n) (set! show-err n))
(def (set-α α_) (set! α α_))
(def (set-batch-size x) (set! batch-size x))

(def (ntw-layer ntw n-layer)
  (if (eq? (car ntw) 'bp1)
    (list-ref (cdr ntw) (- n-layer 2))
    (list-ref ntw (- n-layer 2))))

(def (ntw-node ntw n-layer n)
  (list-ref (ntw-layer ntw n-layer)
            (- n 1)))

(def (ntw-w ntw n-layer n prev-n)
  (list-ref (ntw-node ntw n-layer n) prev-n))

(def (final-output o)
  (first (reverse o)))
; }}}

; 构建网络 {{{
; 以list表示权值
(def (make-network nums)
  (def (make-w number)
    (def generate-w random)
    (def (iter n res)
      (if (= n 0) res (iter (- n 1) (cons (generate-w) res))))
    (reverse (iter number null)))

  (def (iter nums res)

    (def (iter_ num-of-w num-of-nodes res)
      (if (= num-of-nodes 0)
        res
        (iter_ num-of-w (- num-of-nodes 1) (cons (make-w num-of-w) res))))

    (if (eq? (cdr nums) null)
      res
      (iter (cdr nums)
            (cons
              (let ([layer (map (λ (node)
                                  (let ([sum (apply + node)])
                                    (map (λ (elem) (/ elem sum)) node)))
                                (iter_ (car nums) (cadr nums) null))])
                layer)
              res))))
  (cons 'bp1 (reverse (iter nums null))))

(def (type? obj type)
  (eq? (car obj) type))
; }}}

; apply-network {{{
; ReturnValue: 返回的是每个节点的输出值
; 神经元的层从左到右从0开始编号，每一层从上到下从0开始编号
; 权值记为w由该层编号，神经元编号，上层神经元编号共同确定

; af: activity function
(def (apply-network input network #:act-f [af sigmoid])
  (def (iter ntw res)
    (if (null? ntw)
      (reverse res)
      (iter (cdr ntw)
            (cons
              (map af
                   (map (λ (node) (dot-prod node (car res)))
                        (car ntw)))
              res))))
  (if (not (pair? (car network)))
    (iter (cdr network ) (list (map af input)))
    (iter network (list (map af input)))))

; }}}

; train {{{
; alpha指的是学习率
; t表示真实值向量
; input 应该为二维的list，即训练对象
(def train
  (λ (input t network
            #:learning-rate [alpha 0.9]
            #:act-f [af sigmoid]
            #:precision [p 0.000000000001])
    (cond [(type? network 'bp1) (train-bp1 input t (cdr network)
                                           #:learning-rate alpha
                                           #:act-f af
                                           #:precision p)])))

; }}}

; common {{{

(def sigmoid (λ (z) (/ 1 (+ 1 (exp (- 0.0 z))))))
; dot-prod {{{
(def (dot-prod l1 l2)
  (when (not (= (len l1) (len l2)))
    (displayln "dot-prod: l1 and l2 not aligned")
    (displayln "l1: ")
    (displayln l1)
    (displayln "l2: ")
    (displayln l2))
  (foldl +
         0
         (map * l1 l2)))
; }}}
(def (transpose lst-2d)
  (if (null? (car lst-2d))
    null
    (cons (map first lst-2d)
          (transpose (map rest lst-2d)))))

; 定义#f为0权值网络
(def (add-network ntw1 ntw2)
  (cond [(not ntw1) ntw2]
        [(not ntw2) ntw1]
        [else
          (map (λ (layer d-layer)
                 (map (λ (lst d-lst)
                        (map +
                             lst
                             d-lst))
                      layer
                      d-layer))
               ntw1
               ntw2)]))
(def (scale k lst)
  (map (λ (x) (* k x)) lst))
(def remove-head cdr)
(def (remove-tail lst)
  (reverse (rest (reverse lst))))
(def (repeat x n)
  (if (= n 0)
    null
    (cons x (repeat x (- n 1)))))
(def (loss-func y t)
  (/ (square (- y t))
     2))
(def (square x) (* x x))
; }}}
  \end{lstlisting}
\end{document}
